\documentclass{beamer}
% \usetheme{metropolis}
\usetheme{Boadilla}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{tabularx} % Adicionado para tabelas ajustáveis
\usepackage{booktabs}  % Para linhas horizontais de qualidade
\usepackage{listings}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usetikzlibrary{shapes.geometric, arrows}
\lstset{basicstyle=\ttfamily\small, breaklines=true}
\usetikzlibrary{calc}
\usetikzlibrary{positioning, arrows.meta, shapes.geometric} % Para posicionamento, setas e formas
\usepackage{amsfonts}   % Para fontes matemáticas adicionais, se necessário
% \usetheme{Madrid} % Você pode escolher outro tema que preferir
\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=2.5cm, minimum height=1cm,text centered, draw=black, fill=gray!20]
\tikzstyle{process} = [rectangle, minimum width=2.5cm, minimum height=1cm, text centered, draw=black, fill=orange!20]
\tikzstyle{decision} = [diamond, aspect=2, minimum width=2.5cm, minimum height=1cm, text centered, draw=black, fill=green!20]
\tikzstyle{arrow} = [thick,->,>=stealth]

\lstset{
  language=Python,
  basicstyle=\ttfamily\footnotesize,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  showstringspaces=false,
  columns=fullflexible,
  keepspaces=true,
  numbers=left,
  numberstyle=\tiny\color{gray},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=4,
}

\lstset{
    inputencoding=utf8,
    extendedchars=true,
    literate=
    {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
    {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
    {à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
    {À}{{\`A}}1 {È}{{\`E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
    {ã}{{\~a}}1 {õ}{{\~o}}1
    {Ã}{{\~A}}1 {Õ}{{\~O}}1
    {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
    {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
    {ç}{{\c c}}1 {Ç}{{\c C}}1
    {ü}{{\"u}}1 {Ü}{{\"U}}1
}

\title{Aprendizado Não Supervisionado (Aula 01)}
\author{Nator Junior C. Costa}
\date{\today}

\begin{document}

\frame{\titlepage}




\begin{frame}{Introdução ao Aprendizado Não Supervisionado}
    \begin{itemize}
        \item \textbf{Definição:} 
        \begin{itemize}
            \item Método de aprendizado de máquina que identifica padrões e estruturas em dados sem rótulos pré-definidos.
        \end{itemize}
        \item \textbf{Objetivo:}
        \begin{itemize}
            \item Descobrir a estrutura subjacente dos dados.
            \item Agrupar dados semelhantes.
            \item Reduzir a dimensionalidade dos dados para facilitar a visualização e interpretação.
        \end{itemize}
    \end{itemize}
\end{frame}



\begin{frame}{Diferenças}
    \scriptsize % Reduz ainda mais o tamanho da fonte

    \begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash}p{3cm} 
                                   >{\raggedright\arraybackslash}p{4cm} 
                                   >{\raggedright\arraybackslash}p{4cm}}
        \toprule
        \textbf{Característica} & \textbf{Supervisionado} & \textbf{Não Supervisionado} \\
        \midrule
        Dados & 
        \begin{itemize}
            \item Rotulados
        \end{itemize} & 
        \begin{itemize}
            \item Não rotulados
        \end{itemize} \\
        
        Objetivo & 
        \begin{itemize}
            \item Previsão
            \item Classificação
        \end{itemize} & 
        \begin{itemize}
            \item Descoberta de padrões
            \item Estruturação de dados
        \end{itemize} \\
        
        Exemplos de Algoritmos & 
        \begin{itemize}
            \item Regressão
            \item Árvores de Decisão
            \item SVM
        \end{itemize} & 
        \begin{itemize}
            \item K-Means
            \item PCA
            \item DBSCAN
        \end{itemize} \\
        \bottomrule
    \end{tabularx}
    
    \vspace{0.3cm}
    
    \textbf{Quando Usar:}
    \begin{itemize}
        \item \textbf{Supervisionado:} Quando há dados rotulados disponíveis.
        \item \textbf{Não Supervisionado:} Quando não há rótulos e deseja-se explorar a estrutura dos dados.
    \end{itemize}
\end{frame}






\begin{frame}{Aplicações e Exemplos de Aprendizado Não Supervisionado}
    \textbf{Principais Aplicações:}
    \begin{itemize}
        \item \textbf{Clustering (Agrupamento):} Identificação de grupos semelhantes dentro dos dados.
        \item \textbf{Redução de Dimensionalidade:} Simplificação dos dados mantendo as informações essenciais.
        \item \textbf{Detecção de Anomalias:} Identificação de padrões ou pontos de dados que diferem significativamente dos demais.
        \item \textbf{Regras de Associação:} Descoberta de relações entre variáveis nos dados.
    \end{itemize}
\end{frame}


\begin{frame}[fragile]{Aprendizagem Supervisionada}
    \begin{itemize}
        \item A \textbf{aprendizagem supervisionada} utiliza um conjunto de dados rotulados para ensinar o modelo a prever saídas \textbf{y} com base em entradas \textbf{X}.
        \item Cada exemplo no conjunto de dados é um par \textbf{(X, y)}, onde:
        \begin{itemize}
            \item \textbf{X}: Conjunto de características (por exemplo, idade, altura, etc.).
            \item \textbf{y}: Rótulo ou resultado esperado (por exemplo, categoria, classe, etc.).
        \end{itemize}
        \item O objetivo é treinar um modelo para aprender a relação entre \textbf{X} e \textbf{y}, de forma a generalizar para novos dados.
    \end{itemize}
\end{frame}


\begin{frame}{Aprendizagem Supervisionada}
    \begin{itemize}
        \item \textbf{Dados:} Conjunto de pares \(\{ (\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \dots, (\mathbf{x}_n, y_n) \}\), onde:
        \begin{itemize}
            \item \(\mathbf{x}_i \in \mathbb{R}^d\): Vetor de características (e.g., idade, altura).
            \item \(y_i\): Rótulo ou saída esperada (e.g., classe, valor numérico).
        \end{itemize}
        \item \textbf{Objetivo:} Encontrar uma função \(f: \mathbb{R}^d \rightarrow \mathbb{R}\) tal que \(f(\mathbf{x}_i) \approx y_i\).
    \end{itemize}
    
    \vspace{0.5cm}
    
    \begin{center}
        \begin{tikzpicture}[node distance=2.5cm]
            \node (input) [draw, fill=blue!20, rectangle, minimum width=2.5cm, minimum height=1cm] {Entradas  \(\mathbf{X}\)};
            \node (model) [draw, fill=green!20, rectangle, right of=input, xshift=2cm, minimum width=2.5cm, minimum height=1cm] {Modelo \(f(\mathbf{X})\)};
            \node (output) [draw, fill=orange!20, rectangle, right of=model, xshift=2cm, minimum width=2.5cm, minimum height=1cm] {Saída \(\hat{y}\)};
            \draw[->, thick] (input) -- (model) node[midway, above] {};
            \draw[->, thick] (model) -- (output) node[midway, above] {};
            \node at ($(input.east)!0.5!(model.west)$) [above] {Treinamento};
            \node at ($(model.east)!0.5!(output.west)$) [above] {Previsão};
        \end{tikzpicture}
    \end{center}
\end{frame}

\begin{frame}{Aprendizagem Não Supervisionada}
    \begin{itemize}
        \item \textbf{Dados:} Conjunto de vetores \(\{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n\}\), onde \(\mathbf{x}_i \in \mathbb{R}^d\).
        \item \textbf{Objetivo:} Descobrir estruturas ocultas sem rótulos.
    \end{itemize}
    \vspace{0.5cm}
    \begin{columns}[T]
        \column{0.5\textwidth}
        \centering
        \textbf{Clustering}
        \begin{itemize}
            \item Agrupamento de dados similares.
        \end{itemize}
        \begin{tikzpicture}[scale=0.8]
            % Cluster 1
            \fill[red] (0,0) circle (0.1);
            \fill[red] (0.3,0.2) circle (0.1);
            \fill[red] (-0.2,0.1) circle (0.1);
            % Cluster 2
            \fill[blue] (2,2) circle (0.1);
            \fill[blue] (2.2,1.8) circle (0.1);
            \fill[blue] (1.8,2.1) circle (0.1);
            % Circles around clusters
            \draw[red, thick] (0.03,0.1) circle (0.5);
            \draw[blue, thick] (2,2) circle (0.5);
        \end{tikzpicture}
        \column{0.5\textwidth}
        \centering
        \textbf{Redução de Dimensionalidade}
        \begin{itemize}
            \item Projeção em dimensões menores.
        \end{itemize}
        \begin{tikzpicture}[scale=0.8]
            % Axes
            \draw[->] (0,0) -- (3,0) node[right] {$x_1$};
            \draw[->] (0,0) -- (0,2) node[above] {$x_2$};
            % Data points
            \fill (0.5,1.5) circle (0.07);
            \fill (1,1) circle (0.07);
            \fill (1.5,0.5) circle (0.07);
            % Principal Component
            \draw[->, dashed] (0,0) -- (2,2) node[right] {PC1};
            % Projection lines
            \draw[dashed] (0.5,1.5) -- (1,1);
            \draw[dashed] (1,1) -- (1.5,0.5);
        \end{tikzpicture}
    \end{columns}

    \begin{itemize}
        \item Na aprendizagem não supervisionada, temos somente o vetor \textbf{X}.
        \item Nesse caso, o objetivo é descobrir alguma coisa a respeito dos dados.
        \begin{itemize}
            \item Por exemplo, como eles estão agrupados.
        \end{itemize}
    \end{itemize}
\end{frame}


\begin{frame}
    \centering
    \huge
    Agrupamento (Clustering)
\end{frame}


% Frame: Agrupamento (Clustering)
\begin{frame}{Agrupamento (Clustering)}
    \begin{itemize}
        \item \textbf{Definição:} Método de aprendizagem não supervisionada que agrupa dados em clusters baseados em similaridades.
        \item \textbf{Objetivo:} Maximizar a homogeneidade dentro dos clusters e a heterogeneidade entre eles.
        \item \textbf{Aplicações:}
        \begin{itemize}
            \item Análise de imagens
            \item Agrupamento de documentos
        \end{itemize}
    \end{itemize}
    \vspace{0.5cm}
    \begin{center}
        \begin{tikzpicture}[scale=0.8]
            % Cluster 1
            \fill[red!70] (0,0) circle (0.1);
            \fill[red!70] (0.3,0.2) circle (0.1);
            \fill[red!70] (-0.2,0.1) circle (0.1);
            % Cluster 2
            \fill[blue!70] (2,2) circle (0.1);
            \fill[blue!70] (2.2,1.8) circle (0.1);
            \fill[blue!70] (1.8,2.1) circle (0.1);
            % Cluster 3
            \fill[green!70!black] (4,0) circle (0.1);
            \fill[green!70!black] (4.2,0.2) circle (0.1);
            \fill[green!70!black] (3.8,0.1) circle (0.1);
            % Circles around clusters
            \draw[red!70, thick] (0.03,0.1) circle (0.5);
            \draw[blue!70, thick] (2,2) circle (0.5);
            \draw[green!70!black, thick] (4,0.1) circle (0.5);
        \end{tikzpicture}
    \end{center}
\end{frame}


% Frame: K-médias (K-means) - Introdução
\begin{frame}{K-médias (K-means)}
    \begin{itemize}
        \item \textbf{Definição:} Algoritmo que agrupa \( n \) observações em \( k \) clusters, onde cada observação pertence ao cluster com o centroide mais próximo.
        \item \textbf{Objetivo:} Minimizar a soma das distâncias quadráticas dentro dos clusters:
        \[
        \min_{\{C_j\}} \sum_{j=1}^{k} \sum_{\mathbf{x}_i \in C_j} \|\mathbf{x}_i - \boldsymbol{\mu}_j\|^2
        \]
        onde \( \boldsymbol{\mu}_j \) é o centroide do cluster \( C_j \).
    \end{itemize}
     \vspace{-0.5cm}
    \begin{center}
        \begin{tikzpicture}[scale=0.7]
            % Plano cartesiano fixo
            \draw[step=1cm,gray,very thin] (0,0) grid (5,5);
            \draw[->] (0,0) -- (5.2,0) node[right] {$x$};
            \draw[->] (0,0) -- (0,5.2) node[above] {$y$};

            % Dados não rotulados
            \foreach \x/\y in {%
                0.8/3.5,%
                1.2/4,%
                1.5/3,%
                2/2.5,%
                2/1,%
                2.5/2,%
                3/1,%
                3.5/0.8,%
                4/1.5%
            } {%
                \fill[gray!70] (\x,\y) circle (0.08);%
            }
        \end{tikzpicture}
    \end{center}
\end{frame}


% Frame: K-médias - Pseudocódigo (Parte 1)
\begin{frame}[fragile]{K-médias: Pseudocódigo}
    \textbf{Entrada:}
    \begin{itemize}
        \item Conjunto de dados: \( \{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n\} \)
        \item Número de clusters: \( k \)
    \end{itemize}
    \vspace{0.3cm}
    \textbf{Algoritmo:}
    \begin{lstlisting}[language=Python, mathescape=true]
1. Inicializar centroides $\boldsymbol{\mu}_1, \boldsymbol{\mu}_2, \dots, \boldsymbol{\mu}_k$ escolhendo $k$ pontos aleatórios do conjunto de dados.
    \end{lstlisting}
    \vspace{0.3cm}
    \textbf{Repetir até convergência:}
    \begin{lstlisting}[language=Python, mathescape=true]
2. Para cada ponto $\mathbf{x}_i$ do conjunto de dados:
       a) Calcular a distância entre $\mathbf{x}_i$ e cada centroide $\boldsymbol{\mu}_j$.
       b) Atribuir $\mathbf{x}_i$ ao cluster $C_j$ com o centroide mais próximo.
    \end{lstlisting}
\end{frame}

% Frame: K-médias - Pseudocódigo (Parte 2)
\begin{frame}[fragile]{K-médias: Pseudocódigo (continuação)}
    \textbf{Continuando o algoritmo:}
    \begin{lstlisting}[language=Python, mathescape=true]
3. Para cada cluster $C_j$:
       a) Atualizar o centroide $\boldsymbol{\mu}_j$ calculando a média dos pontos atribuídos ao cluster:
          $\boldsymbol{\mu}_j = \frac{1}{|C_j|} \sum_{\mathbf{x}_i \in C_j} \mathbf{x}_i$
    \end{lstlisting}
    \vspace{0.3cm}
    \textbf{Condição de Parada:}
    \begin{itemize}
        \item O algoritmo para quando os centroides não mudam significativamente entre as iterações, ou após um número máximo de iterações.
    \end{itemize}
    \vspace{0.3cm}
    \textbf{Saída:}
    \begin{itemize}
        \item Clusters \( C_1, C_2, \dots, C_k \) com seus respectivos centroides \( \boldsymbol{\mu}_1, \boldsymbol{\mu}_2, \dots, \boldsymbol{\mu}_k \).
    \end{itemize}
\end{frame}



% Frame: Detalhamento das Funções no Pseudocódigo
\begin{frame}[fragile]{Detalhamento das Funções}
    \textbf{Cálculo da Distância Euclidiana:}
    \begin{equation*}
    d(\mathbf{x}_i, \boldsymbol{\mu}_j) = \sqrt{\sum_{l=1}^{m} (x_{il} - \mu_{jl})^2}
    \end{equation*}
    onde \( m \) é o número de características.

    \vspace{0.5cm}

    \textbf{Atualização dos Centroides:}
    \begin{equation*}
    \boldsymbol{\mu}_j = \frac{1}{|C_j|} \sum_{\mathbf{x}_i \in C_j} \mathbf{x}_i
    \end{equation*}

    \vspace{0.5cm}

    \textbf{Critério de Convergência:}
    \begin{itemize}
        \item Pode ser definido por um limiar \( \epsilon \) tal que:
        \[
        \max_j \|\boldsymbol{\mu}_j^{(t)} - \boldsymbol{\mu}_j^{(t-1)}\| < \epsilon
        \]
        \item Ou fixar um número máximo de iterações.
    \end{itemize}
\end{frame}




% Frame: K-médias - Passo 1: Inicialização dos Centroides
\begin{frame}{K-médias: Passo 1 - Inicialização dos Centroides}
    \begin{itemize}
        \item \textbf{Passo 1:} Selecionar aleatoriamente \( k \) centroides iniciais \( \boldsymbol{\mu}_1, \boldsymbol{\mu}_2 \).
        \item Neste exemplo, \( k = 2 \).
    \end{itemize}
    \vspace{0.3cm}
    \begin{center}
        \begin{tikzpicture}[scale=0.9]
            % Plano cartesiano fixo
            \draw[step=1cm,gray,very thin] (0,0) grid (5,5);
            \draw[->] (0,0) -- (5.2,0) node[right] {$x$};
            \draw[->] (0,0) -- (0,5.2) node[above] {$y$};

            % Dados
            \foreach \x/\y in {%
                0.8/3.5,%
                1.2/4,%
                1.5/3,%
                2/2.5,%
                2/1,%
                2.5/2,%
                3/1,%
                3.5/0.8,%
                4/1.5%
            } {%
                \fill[gray!70] (\x,\y) circle (0.08);%
            }
            % Centroides iniciais
            \fill[red] (1,4) node[above] {\(\boldsymbol{\mu}_1\)} circle (0.12);
            \fill[blue] (4,1.5) node[right] {\(\boldsymbol{\mu}_2\)} circle (0.12);
        \end{tikzpicture}
    \end{center}
\end{frame}

% Frame: K-médias - Passo 2: Atribuição aos Clusters
\begin{frame}{K-médias: Passo 2 - Atribuição aos Clusters}
    \begin{itemize}
        \item \textbf{Passo 2:} Atribuir cada ponto \( \mathbf{x}_i \) ao cluster com o centroide mais próximo:
        \[
        c_i = \arg\min_{j} \|\mathbf{x}_i - \boldsymbol{\mu}_j\|^2
        \]
    \end{itemize}
    \vspace{0.3cm}
    \begin{center}
        \begin{tikzpicture}[scale=0.9]
            % Plano cartesiano fixo
            \draw[step=1cm,gray,very thin] (0,0) grid (5,5);
            \draw[->] (0,0) -- (5.2,0) node[right] {$x$};
            \draw[->] (0,0) -- (0,5.2) node[above] {$y$};

            % Centroides
            \fill[red] (1,4) node[above] {\(\boldsymbol{\mu}_1\)} circle (0.12);
            \fill[blue] (4,1.5) node[right] {\(\boldsymbol{\mu}_2\)} circle (0.12);

            % Dados atribuídos e linhas de conexão
            \foreach \x/\y/\c/\mx/\my in {%
                0.8/3.5/red/1/4,%
                1.2/4/red/1/4,%
                1.5/3/red/1/4,%
                2/2.5/red/1/4,%
                2/1/blue/4/1.5,%
                2.5/2/blue/4/1.5,%
                3/1/blue/4/1.5,%
                3.5/0.8/blue/4/1.5,%
                4/1.5/blue/4/1.5%
            } {%
                \fill[\c!70] (\x,\y) circle (0.08);%
                \draw[dashed, \c!50] (\x,\y) -- (\mx,\my);%
            }
        \end{tikzpicture}
    \end{center}
\end{frame}

% Frame: K-médias - Passo 3: Atualização dos Centroides
\begin{frame}{K-médias: Passo 3 - Atualização dos Centroides}
    \begin{itemize}
        \item \textbf{Passo 3:} Atualizar os centroides calculando a média dos pontos atribuídos:
        \[
        \boldsymbol{\mu}_j = \frac{1}{|C_j|} \sum_{\mathbf{x}_i \in C_j} \mathbf{x}_i
        \]
    \end{itemize}
    \vspace{0.3cm}
    \begin{center}
        \begin{tikzpicture}[scale=0.9]
            % Plano cartesiano fixo
            \draw[step=1cm,gray,very thin] (0,0) grid (5,5);
            \draw[->] (0,0) -- (5.2,0) node[right] {$x$};
            \draw[->] (0,0) -- (0,5.2) node[above] {$y$};

            % Dados atribuídos
            \foreach \x/\y/\c in {%
                0.8/3.5/red,%
                1.2/4/red,%
                1.5/3/red,%
                2/2.5/red,%
                2/1/blue,%
                2.5/2/blue,%
                3/1/blue,%
                3.5/0.8/blue,%
                4/1.5/blue%
            } {%
                \fill[\c!70] (\x,\y) circle (0.08);%
            }

            % Centroides atualizados
            \fill[red] (1.375,3.125) node[above] {\(\boldsymbol{\mu}_1'\)} circle (0.12);
            \fill[blue] (3.2,1.16) node[right] {\(\boldsymbol{\mu}_2'\)} circle (0.12);

            % Linhas de atualização
            \draw[->, red!70] (1,4) -- (1.375,3.125);
            \draw[->, blue!70] (4,1.5) -- (3.2,1.16);
        \end{tikzpicture}
    \end{center}
\end{frame}

% Frame: K-médias - Repetição dos Passos 2 e 3
\begin{frame}{K-médias: Repetição dos Passos 2 e 3}
    \begin{itemize}
        \item Repetir os passos de atribuição e atualização até a convergência.
    \end{itemize}
    \vspace{0.3cm}
    \begin{center}
        % Iteração 2
        \only<1>{
            \begin{tikzpicture}[scale=0.9]
                % Plano cartesiano fixo
                \draw[step=1cm,gray,very thin] (0,0) grid (5,5);
                \draw[->] (0,0) -- (5.2,0) node[right] {$x$};
                \draw[->] (0,0) -- (0,5.2) node[above] {$y$};

                % Centroides atualizados
                \fill[red] (1.375,3.125) node[above] {\(\boldsymbol{\mu}_1'\)} circle (0.12);
                \fill[blue] (3.2,1.16) node[right] {\(\boldsymbol{\mu}_2'\)} circle (0.12);

                % Dados atribuídos e linhas
                \foreach \x/\y/\c/\mx/\my in {%
                    0.8/3.5/red/1.375/3.125,%
                    1.2/4/red/1.375/3.125,%
                    1.5/3/red/1.375/3.125,%
                    2/2.5/red/1.375/3.125,%
                    2/1/blue/3.2/1.16,%
                    2.5/2/blue/3.2/1.16,%
                    3/1/blue/3.2/1.16,%
                    3.5/0.8/blue/3.2/1.16,%
                    4/1.5/blue/3.2/1.16%
                } {%
                    \fill[\c!70] (\x,\y) circle (0.08);%
                    \draw[dashed, \c!50] (\x,\y) -- (\mx,\my);%
                }
            \end{tikzpicture}
        }

        % Iteração 3
        \only<2>{
            \begin{tikzpicture}[scale=0.9]
                % Plano cartesiano fixo
                \draw[step=1cm,gray,very thin] (0,0) grid (5,5);
                \draw[->] (0,0) -- (5.2,0) node[right] {$x$};
                \draw[->] (0,0) -- (0,5.2) node[above] {$y$};

                % Centroides atualizados novamente
                \fill[red] (1.375,3.125) node[above] {\(\boldsymbol{\mu}_1\)} circle (0.12);
                \fill[blue] (3.2,1.16) node[right] {\(\boldsymbol{\mu}_2\)} circle (0.12);

                % Dados atribuídos (mesmos da iteração anterior)
                \foreach \x/\y/\c in {%
                    0.8/3.5/red,%
                    1.2/4/red,%
                    1.5/3/red,%
                    2/2.5/red,%
                    2/1/blue,%
                    2.5/2/blue,%
                    3/1/blue,%
                    3.5/0.8/blue,%
                    4/1.5/blue%
                } {%
                    \fill[\c!70] (\x,\y) circle (0.08);%
                }
            \end{tikzpicture}
            \vspace{0.2cm}
            \centering
            \\
            \textit{Centroides não mudaram significativamente: convergência alcançada}
        }
    \end{center}
\end{frame}

% Frame: K-médias - Resultado Final
\begin{frame}{K-médias: Resultado Final}
    \begin{itemize}
        \item \textbf{Clusters Definidos:} Os pontos foram agrupados com sucesso.
        \item \textbf{Centroides Estáveis:} O algoritmo convergiu.
    \end{itemize}
    \vspace{0.3cm}
    \begin{center}
        \begin{tikzpicture}[scale=0.9]
            % Plano cartesiano fixo
            \draw[step=1cm,gray,very thin] (0,0) grid (5,5);
            \draw[->] (0,0) -- (5.2,0) node[right] {$x$};
            \draw[->] (0,0) -- (0,5.2) node[above] {$y$};

            % Centroides finais
            \fill[red] (1.375,3.125) node[above] {\(\boldsymbol{\mu}_1\)} circle (0.12);
            \fill[blue] (3.2,1.16) node[right] {\(\boldsymbol{\mu}_2\)} circle (0.12);

            % Dados finais atribuídos
            \foreach \x/\y/\c in {%
                0.8/3.5/red,%
                1.2/4/red,%
                1.5/3/red,%
                2/2.5/red,%
                2/1/blue,%
                2.5/2/blue,%
                3/1/blue,%
                3.5/0.8/blue,%
                4/1.5/blue%
            } {%
                \fill[\c!70] (\x,\y) circle (0.08);%
            }
        \end{tikzpicture}
    \end{center}
\end{frame}





\begin{frame}{Resumo K-Means}
    \textbf{Algoritmo K-Means:}
    \begin{enumerate}
        \item Agrupa os dados em \(k\) grupos, onde \(k\) é predefinido.
        \item Seleciona \(k\) pontos aleatoriamente como centros de cluster (centróides).
        \item Atribui objetos ao centro de cluster mais próximo de acordo com a função de distância euclidiana.
        \item Calcula o centróide ou a média de todos os objetos em cada cluster.
        \item Repete as etapas 3 e 4 até que os mesmos pontos sejam atribuídos a cada cluster em rodadas consecutivas (convergência).
    \end{enumerate}
    \vspace{0.5cm}
    O K-Means é amplamente utilizado para agrupar dados e identificar padrões, separando-os em grupos homogêneos.
\end{frame}


\begin{frame}
    \centering
    \huge
    Um exemplo numérico
\end{frame}




\begin{frame}{Iteração 1 - Inicializando os Centroides}
    \textbf{Exemplo:} Suponha que temos os seguintes dados de idade:

    \begin{block}{Dados}
    \[
    15, 15, 16, 19, 19, 20, 20, 21, 22, 28, 35, 40, 41, 42, 43, 44, 60, 61, 65
    \]
    \end{block}

    Inicializamos dois centroides aleatórios:
    \[
    c_1 = 16, \quad c_2 = 22
    \]

    Agora vamos calcular as distâncias para cada ponto e atribuí-los aos clusters.
\end{frame}


\begin{frame}{Iteração 1 - Atribuindo ao Cluster Mais Próximo}
    \textbf{Passo 1:} Calculando as distâncias e atribuindo os pontos.

    \begin{itemize}
        \item Para \(x = 15\):
        \[
        d(x, c_1) = |15 - 16| = 1 \quad d(x, c_2) = |15 - 22| = 7
        \]
        \item O ponto \(15\) será atribuído ao cluster 1 (\(c_1\)).
    \end{itemize}

    \begin{itemize}
        \item Repetimos este processo para todos os pontos...
    \end{itemize}

    \only<2>{
        \textbf{Resultado da Atribuição na Iteração 1:}
        \begin{block}{Clusters}
        Cluster 1 (\(c_1 = 16\)): 15, 15, 16, 19, 19, 20, 20, 21 \\
        Cluster 2 (\(c_2 = 22\)): 22, 28, 35, 40, 41, 42, 43, 44, 60, 61, 65
        \end{block}
    }
\end{frame}


\begin{frame}{Iteração 1 - Atualizando os Centroides}
    \textbf{Passo 2:} Atualizando os centroides.

    Calculamos a média dos pontos atribuídos a cada cluster para determinar os novos centroides:

    \[
    \text{Novo } c_1 = \frac{15 + 15 + 16 + 19 + 19 + 20 + 20 + 21}{8} = 18
    \]
    \[
    \text{Novo } c_2 = \frac{22 + 28 + 35 + 40 + 41 + 42 + 43 + 44 + 60 + 61 + 65}{11} = 43
    \]

    Os novos centroides para a próxima iteração são:
    \[
    c_1 = 18, \quad c_2 = 43
    \]
\end{frame}


\begin{frame}{Iteração 2 - Atribuindo ao Cluster Mais Próximo}
    \textbf{Iteração 2:} Atribuindo os pontos aos novos centroides.

    Vamos repetir o cálculo de distâncias com os novos centroides (\(c_1 = 18\) e \(c_2 = 43\)):

    \begin{itemize}
        \item Para \(x = 15\):
        \[
        d(x, c_1) = |15 - 18| = 3 \quad d(x, c_2) = |15 - 43| = 28
        \]
        \item O ponto \(15\) ainda será atribuído ao cluster 1 (\(c_1\)).
    \end{itemize}

    \only<2>{
        \textbf{Resultado da Atribuição na Iteração 2:}
        \begin{block}{Clusters}
        Cluster 1 (\(c_1 = 18\)): 15, 15, 16, 19, 19, 20, 20, 21, 22, 28 \\
        Cluster 2 (\(c_2 = 43\)): 35, 40, 41, 42, 43, 44, 60, 61, 65
        \end{block}
    }
\end{frame}


\begin{frame}{Iteração 2 - Atualizando os Centroides}
    \textbf{Passo 2:} Atualizando os centroides.

    Calculamos novamente a média dos pontos para atualizar os centroides:

    \[
    \text{Novo } c_1 = \frac{15 + 15 + 16 + 19 + 19 + 20 + 20 + 21 + 22 + 28}{10} = 19.5
    \]
    \[
    \text{Novo } c_2 = \frac{35 + 40 + 41 + 42 + 43 + 44 + 60 + 61 + 65}{9} = 47.88
    \]

    Os novos centroides para a próxima iteração são:
    \[
    c_1 = 19.5, \quad c_2 = 47.88
    \]

    Repetimos esse processo até que os centroides parem de mudar significativamente (convergência).
\end{frame}





\begin{frame}
    \centering
    \huge
    Google Colab\\


    \small
    http://u.acilab.com.br/WP
        
\end{frame}









\begin{frame}
    \centering
    \huge
    Implementação em Python 
\end{frame}

% Slide: Etapas do Algoritmo K-Means
\begin{frame}{Etapas do Algoritmo K-Means}
    \begin{enumerate}
        \item \textbf{Inicialização dos Centroides:}
        \begin{itemize}
            \item Selecionar aleatoriamente \( k \) pontos do conjunto de dados como os centroides iniciais.
            \item Função relevante: \texttt{inicializar\_centroides}
        \end{itemize}

        \vspace{0.3cm}

        \item \textbf{Cálculo das Distâncias:}
        \begin{itemize}
            \item Calcular a distância euclidiana entre cada ponto e os centroides.
            \item Função relevante: \texttt{distancia}
        \end{itemize}

        \vspace{0.3cm}

        \item \textbf{Atribuição aos Clusters:}
        \begin{itemize}
            \item Atribuir cada ponto ao cluster cujo centróide esteja mais próximo.
            \item Função relevante: \texttt{fit} (parte que atribui rótulos)
        \end{itemize}

        \vspace{0.3cm}

        \item \textbf{Atualização dos Centroides:}
        \begin{itemize}
            \item Atualizar os centroides, calculando a média dos pontos pertencentes a cada cluster.
            \item Função relevante: \texttt{updateCenters}
        \end{itemize}

        \vspace{0.3cm}

        \item \textbf{Verificação da Convergência:}
        \begin{itemize}
            \item Verificar se os centroides pararam de mudar. Se sim, o algoritmo é interrompido.
            \item Função relevante: Lógica interna do \texttt{fit}
        \end{itemize}

        \vspace{0.3cm}

        \item \textbf{Classificação de Novos Pontos:}
        \begin{itemize}
            \item Após o treinamento, atribuir novos pontos ao cluster mais próximo usando os centroides finais.
            \item Função relevante: \texttt{predict}
        \end{itemize}
    \end{enumerate}
\end{frame}


% Frame: Implementação do KMeans - Inicialização
\begin{frame}[fragile]{Implementação do KMeans - Inicialização}
\begin{lstlisting}[language=Python]
class Kmeans:
    def __init__(self, k=2):
        self.k = k
        self.centroids = []
\end{lstlisting}
    \vspace{0.3cm}
    \begin{itemize}
        \item A função \texttt{init} inicializa o número de clusters \( k \) e uma lista vazia para armazenar os centroides.
    \end{itemize}
\end{frame}

% Frame: Implementação do KMeans - Cálculo da Distância
\begin{frame}[fragile]{Implementação do KMeans - Cálculo da Distância}
\begin{lstlisting}[language=Python]
    def distancia(self, x1, x2):
        return np.sqrt(np.sum((x1 - x2)**2))
\end{lstlisting}
    \vspace{0.3cm}
    \begin{itemize}
        \item A função \texttt{distancia} calcula a distância euclidiana entre dois pontos \( x1 \) e \( x2 \).
    \end{itemize}
    \vspace{0.3cm}
    \begin{itemize}
        \item \textbf{Fórmula da Distância Euclidiana:}
        \[
        d(\mathbf{x}_i, \mathbf{\mu}_j) = \sqrt{\sum_{l=1}^{m} (x_{il} - \mu_{jl})^2}
        \]
        onde:
        \begin{itemize}
            \item \( x_{il} \) é o \( l \)-ésimo componente do ponto \( \mathbf{x}_i \).
            \item \( \mu_{jl} \) é o \( l \)-ésimo componente do centroide \( \mathbf{\mu}_j \).
        \end{itemize}
    \end{itemize}
\end{frame}

% Frame: Implementação do KMeans - Atualização dos Centroides
\begin{frame}[fragile]{Implementação do KMeans - Atualização dos Centroides}
\begin{lstlisting}[language=Python]
    def updateCenters(self, x, y):
        new_clusters = []
        for i in range(self.k):
            indices = np.where(np.array(y) == i)
            new_clusters.append(np.mean(np.array(x)[indices], axis=0))
        return new_clusters
\end{lstlisting}
    \vspace{0.3cm}
    \begin{itemize}
        \item A função \texttt{updateCenters} recalcula os centroides como a média dos pontos atribuídos a cada cluster.
    \end{itemize}
    \vspace{0.3cm}
    \begin{itemize}
        \item \textbf{Fórmula para Atualização do Centroide:}
        \[
        \mathbf{\mu}_j = \frac{1}{|C_j|} \sum_{\mathbf{x}_i \in C_j} \mathbf{x}_i
        \]
        onde:
        \begin{itemize}
            \item \( \mathbf{\mu}_j \) é o centroide do cluster \( j \).
            \item \( C_j \) é o conjunto de pontos atribuídos ao cluster \( j \).
            \item \( |C_j| \) é o número de pontos no cluster \( j \).
        \end{itemize}
    \end{itemize}
\end{frame}

% Frame: Implementação do KMeans - Algoritmo de Treinamento (Fit)
\begin{frame}[fragile]{Algoritmo de Treinamento (Fit)}
\begin{lstlisting}[language=Python]
    def fit(self, x, para_em=10):
        # Inicializa os centróides aleatoriamente
        self.centroids = x[np.random.randint(0, len(x), self.k)]
        count = 0
        while True:
            y = []
            # Atribui cada ponto ao centróide mais próximo
            for i in range(len(x)):
                distancias = [self.distancia(x[i], centroid) for centroid in self.centroids]
                y.append(np.argmin(distancias))
            self.centroids = self.updateCenters(x, y)

            if count > para_em:
                break
            count += 1
        return y
\end{lstlisting}
    \vspace{0.3cm}
    \begin{itemize}
        \item A função \texttt{fit} é responsável pelo treinamento do modelo. 
        %Ela inicializa os centroides, calcula as distâncias e atualiza os clusters até atingir o número máximo de iterações ou convergir.
    \end{itemize}
\end{frame}

% Frame: Predição de Clusters
\begin{frame}[fragile]{Predição de Clusters}
\begin{lstlisting}[language=Python]
def predict(self,x):
    distancias = [self.distancia(x, centroid) for centroid in self.centroids]
    return np.argmax(distancias)
\end{lstlisting}
    \vspace{0.3cm}
    \begin{itemize}
        \item A função \texttt{predict} recebe um novo ponto \( x \) e retorna o cluster ao qual esse ponto pertence, baseado na distância aos centroides.
    \end{itemize}
    \vspace{0.3cm}
    \begin{itemize}
        \item \textbf{Fórmula da Decisão:}
        \[
        \text{Cluster}(x) = \arg\min_{j} \{d(\mathbf{x}, \mathbf{\mu}_j)\}
        \]
        onde \( d(\mathbf{x}, \mathbf{\mu}_j) \) é a distância entre o ponto \( \mathbf{x} \) e o centroide \( \mathbf{\mu}_j \).
    \end{itemize}
\end{frame}




\begin{frame}
    \centering
    \huge
    Avaliação de Desempenho em Clustering
\end{frame}


% Frame: Avaliação de Desempenho em Clustering
\begin{frame}{Avaliação de Desempenho em Clustering}
    \begin{itemize}
        \item \textbf{Desafio:} Avaliar a qualidade dos clusters obtidos sem os rótulos verdadeiros.
        \item \textbf{Objetivos da Avaliação:}
        \begin{itemize}
            \item Medir a \textbf{coerência interna} dos clusters (compactação).
            \item Medir a \textbf{separação} entre clusters distintos.
        \end{itemize}
        \item \textbf{Métricas Comuns:}
        \begin{itemize}
            \item Soma dos Quadrados Dentro dos Clusters (\textbf{Within-Cluster Sum of Squares - WCSS}).
            \item Coeficiente de Silhueta (\textbf{Silhouette Coefficient}).
            % \item Índice de Davies-Bouldin.
            % \item Índice de Calinski-Harabasz.
        \end{itemize}
    \end{itemize}
\end{frame}

% Frame: Soma dos Quadrados Dentro dos Clusters (WCSS)
\begin{frame}{Soma dos Quadrados Dentro dos Clusters (WCSS)}
    \begin{itemize}
        \item \textbf{Definição:} Mede a variabilidade dos pontos dentro de cada cluster.
        \item \textbf{Fórmula:}
        \[
        \text{WCSS} = \sum_{j=1}^{k} \sum_{\mathbf{x}_i \in C_j} \|\mathbf{x}_i - \boldsymbol{\mu}_j\|^2
        \]
        \item \textbf{Interpretação:} Valores menores indicam clusters mais compactos.
    \end{itemize}
    \vspace{0.3cm}
    % \begin{center}
    %     \includegraphics[width=0.6\textwidth]{elbow_method.png}
    % \end{center}
\end{frame}

% Frame: Método do Cotovelo (Elbow Method)
\begin{frame}{Método do Cotovelo}
    \begin{itemize}
        \item \textbf{Objetivo:} Determinar o número ideal de clusters \( k \).
        \item \textbf{Procedimento:}
        \begin{enumerate}
            \item Executar o algoritmo K-médias para diferentes valores de \( k \).
            \item Calcular o WCSS para cada \( k \).
            \item Plotar o WCSS em função de \( k \).
        \end{enumerate}
        \item \textbf{Interpretação:} O ponto onde a redução do WCSS começa a se estabilizar (cotovelo) indica o número ideal de clusters.
    \end{itemize}
    \vspace{0.2cm}
    \begin{center}
        \begin{tikzpicture}
            \begin{axis}[
                xlabel={Número de Clusters \( k \)},
                ylabel={WCSS},
                xmin=1, xmax=10,
                ymin=0, ymax=100,
                xtick={1,2,...,10},
                ytick={0,20,...,100},
                legend pos=north east,
                ymajorgrids=true,
                grid style=dashed,
                width=0.8\textwidth,
                height=5cm,
            ]
            \addplot[
                color=blue,
                mark=*,
                ]
                coordinates {
                (1, 90)
                (2, 70)
                (3, 50)
                (4, 40)
                (5, 35)
                (6, 30)
                (7, 28)
                (8, 27)
                (9, 26)
                (10,25)
                };
            \end{axis}
        \end{tikzpicture}
    \end{center}
\end{frame}

% Frame: Coeficiente de Silhueta
\begin{frame}{Coeficiente de Silhueta}
    \begin{itemize}
        \item \textbf{Definição:} Avalia a qualidade do agrupamento considerando a coesão e separação.
        \item \textbf{Fórmula para um ponto \( \mathbf{x}_i \):}
        \[
        s_i = \frac{b_i - a_i}{\max\{a_i, b_i\}}
        \]
        onde:
        \begin{itemize}
            \item \( a_i \): Distância média entre \( \mathbf{x}_i \) e os outros pontos do mesmo cluster.
            \item \( b_i \): Distância média entre \( \mathbf{x}_i \) e os pontos do cluster mais próximo.
        \end{itemize}
        \item \textbf{Interpretação:}
        \begin{itemize}
            \item \( -1 \leq s_i \leq 1 \).
            \item Valores próximos a 1 indicam que o ponto está bem inserido no cluster.
            \item Valores próximos a -1 indicam possível má classificação.
        \end{itemize}
    \end{itemize}
\end{frame}

% Frame: Exemplo do Coeficiente de Silhueta
\begin{frame}{Exemplo do Coeficiente de Silhueta}
    \begin{itemize}
        \item \textbf{Gráfico de Silhueta:} Mostra a distribuição dos valores de silhueta para cada cluster.
        \item \textbf{Uso:} Auxilia na avaliação do número de clusters e na identificação de pontos mal agrupados.
    \end{itemize}
    % \vspace{0.3cm}
    \begin{center}
        \begin{tikzpicture}
            \begin{axis}[
                xbar,
                xmin=-0.2, xmax=1,
                xlabel={Coeficiente de Silhueta},
                ylabel={Pontos de Dados},
                ytick={1,2,3,4,5,6,7,8,9,10},
                yticklabels={
                    $P_{10}$,
                    $P_{9}$,
                    $P_{8}$,
                    $P_{7}$,
                    $P_{6}$,
                    $P_{5}$,
                    $P_{4}$,
                    $P_{3}$,
                    $P_{2}$,
                    $P_{1}$
                },
                yticklabel style={font=\small},
                width=0.8\textwidth,
                height=6cm,
                bar width=8pt,
                enlarge y limits=0.05,
                enlarge x limits=0.02,
                axis x line=bottom,
                axis y line=left,
                legend style={at={(0.5,-0.15)},anchor=north,legend columns=-1},
                ]
    
                % Cluster 1
                \addplot+[
                    bar shift=0pt,
                    fill=red!70,
                    draw=none,
                ] coordinates {
                    (0.85,10)
                    (0.80,9)
                    (0.75,8)
                    (0.70,7)
                    (0.65,6)
                };
    
                % Cluster 2
                \addplot+[
                    bar shift=0pt,
                    fill=blue!70,
                    draw=none,
                ] coordinates {
                    (0.90,5)
                    (0.88,4)
                    (0.85,3)
                    (0.80,2)
                    (0.78,1)
                };
    
                \legend{Cluster 1, Cluster 2}
            \end{axis}
        \end{tikzpicture}
    \end{center}
\end{frame}














\end{document}
